package flat/compiler

import flat/io
import flat/ast
import flat/compiler/models
import flat/lexer
import flat/parser
import flat/parser/flat/FileParser as FlatFileParser
import flat/parser/java/FileParser as JavaFileParser
import flat/log/Logger
import flat/time/Timer
import flat/eventstream
import flat/fucli
import flat/regex

import flat/regex/RegexStringExtensions

class {
  static Logger log = Logger(Compiler.class)

  static CliArg excludeArg = CliArg("--exclude", ["-x"], count: 1)
  static CliArg threadsArg = CliArg("--threads", ["-t"], count: 1)
  static CliArg languageArg = CliArg("--language", ["--lang"], count: 1)
  static CliArg syncArg = CliArg("--sync")

  public static async main(String[] args) {
    let fucli = FuCli([
      excludeArg,
      threadsArg,
      languageArg,
      syncArg,
    ]):parse(args.skip(2))

    let completeTimer = Timer():start()

    let inputFile = File(args[1])

    let excludes = excludeArg.values.map({ Pattern(_) })
    let extension = languageArg.value ?: "flat"

    if (inputFile.isDirectory) {
      let files = inputFile.getChildFiles(true)
        .filter({ _.extensionName == extension })
        .filterAsync(file => excludes.noneAsync(x => file.getCanonicalPath().matches(x)))

      if (syncArg.enabled) {
        files.forEachAsync({ compileFile(_) })
      } else {
        files.forEachParallel({ compileFile(_) }, maxParallel: threadsArg.enabled ? Int.parseInt(threadsArg.value) : 10)
      }
    } else {
      compileFile(inputFile)
    }

    completeTimer.stop()

    Compiler.log.info("Done! Took #{completeTimer.duration}ms")
  }

  static async compileFile(File sourceFile) {
    let tokenizerTimer = Timer()
    let lexTimer = Timer()
    let parseTimer = Timer()
    let fileTimer = Timer()
    let completeTimer = Timer()

    Compiler.log.info("Compiling file #{sourceFile.nativeLocation}...")

    completeTimer.start()

    fileTimer.start()
    let fileStream = sourceFile.createReadStream()
    fileStream.on("close", { fileTimer.stop() })

    if (syncArg.enabled) fileStream.waitFor("close")

    tokenizerTimer.start()
    var Int tokenCount = 0
    let lexemeStream = Tokenizer().tokenize(fileStream)
    lexemeStream.on<Lexeme>("data", lexeme => Compiler.log.debugFunc({"Lexeme: '#{lexeme.value}'"}))
    lexemeStream.on<Lexeme>("data", { tokenCount++ })
    lexemeStream.on("close", { tokenizerTimer.stop() })
    lexemeStream.emit("start")

    if (syncArg.enabled) lexemeStream.waitFor("close")

    lexTimer.start()
    let tokenStream = Lexer().lexemesToTokens(lexemeStream)
    tokenStream.on<Token>("data", token => Compiler.log.debugFunc({"Token: '#{token.value}'"}))
    tokenStream.on("close", { lexTimer.stop() })
    tokenStream.emit("start")

    if (syncArg.enabled) tokenStream.waitFor("close")

    let rootParser = match languageArg.value {
      "java" => JavaFileParser()
      default => FlatFileParser()
    }

    parseTimer.start()
    let nodeStream = Parser().parse(rootParser, ParseContext(file: sourceFile), tokenStream)
    nodeStream.on("quit", {
      Compiler.log.info("Closing streams for file #{sourceFile.nativeLocation}")
      fileStream.emit("close")
      fileStream.close()
      lexemeStream.emit("close")
      lexemeStream.close()
      tokenStream.emit("close")
      tokenStream.close()
      nodeStream.emit("close")
      nodeStream.close()
    })
    nodeStream.on("data", node => Compiler.log.debugFunc({"Node: #{node}"}))
    nodeStream.on("close", { parseTimer.stop() })
    nodeStream.emit("start")

    if (syncArg.enabled) nodeStream.waitFor("close")

    if (!syncArg.enabled) {
      fileStream.waitFor("close")
      lexemeStream.waitFor("close")
      tokenStream.waitFor("close")
      nodeStream.waitFor("close")
    }

    completeTimer.stop()

    Compiler.log.info("Finished compiling file #{sourceFile.nativeLocation}! Took #{completeTimer.duration}ms (reading file: #{fileTimer.duration}ms, tokenizing #{tokenizerTimer.duration}ms, lexing #{lexTimer.duration}ms, parsing #{parseTimer.duration}ms (#{tokenCount} tokens))")
  }
}
